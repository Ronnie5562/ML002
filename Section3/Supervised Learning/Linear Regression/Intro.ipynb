{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6afe8f",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It assumes that there exists a linear relationship between the dependent variable and the independent variable(s). The goal of linear regression is to find the line of best fit that minimizes the difference between the actual values and the predicted values.\n",
    "\n",
    "There are two types of linear regression:\n",
    "- Simple linear regression: when there is only one independent variable\n",
    "- Multiple linear regression: when there are two or more independent variables\n",
    "\n",
    "Linear regression can be used for various applications, such as predicting sales based on advertising expenses, or predicting the price of a house based on its characteristics (e.g. size, location, number of rooms, etc.).\n",
    "\n",
    "The basic equation for simple linear regression is:\n",
    "\n",
    "y = mx + b\n",
    "\n",
    "where:\n",
    "- y is the dependent variable\n",
    "- x is the independent variable\n",
    "- m is the slope of the line\n",
    "- b is the y-intercept\n",
    "\n",
    "The slope of the line (m) represents the change in y for every unit change in x, while the y-intercept (b) represents the value of y when x is equal to 0.\n",
    "\n",
    "To perform linear regression, various algorithms can be used, such as ordinary least squares (OLS), gradient descent, or Bayesian regression.\n",
    "\n",
    "The mathematical model for linear regression can be written as:\n",
    "\n",
    "y = β0 + β1*x1 + β2*x2 + ... + βn*xn + ε\n",
    "\n",
    "where:\n",
    "- y is the dependent variable\n",
    "- x1, x2, ..., xn are the independent variables\n",
    "- β0, β1, β2, ..., βn are the regression coefficients or weights\n",
    "- ε is the error term\n",
    "\n",
    "The goal of linear regression is to estimate the regression coefficients that best fit the data, by minimizing the sum of the squared differences between the predicted values and the actual values. This is known as the least squares method.\n",
    "\n",
    "To estimate the regression coefficients, we can use the normal equation or gradient descent algorithm. The normal equation is given by:\n",
    "\n",
    "θ = (X^T X)^-1 X^T y\n",
    "\n",
    "where:\n",
    "- θ is a vector of regression coefficients\n",
    "- X is the design matrix, which contains the independent variables\n",
    "- X^T is the transpose of X\n",
    "- y is a vector of dependent variable values\n",
    "\n",
    "The gradient descent algorithm is an iterative optimization algorithm that updates the regression coefficients in the opposite direction of the gradient of the cost function. The cost function measures the difference between the predicted values and the actual values, and is typically defined as the mean squared error.\n",
    "\n",
    "There are different variations of linear regression, including simple linear regression (with one independent variable) and multiple linear regression (with multiple independent variables). Additionally, linear regression can be extended to handle non-linear relationships between the dependent and independent variables, by adding polynomial terms or other transformations of the variables. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
