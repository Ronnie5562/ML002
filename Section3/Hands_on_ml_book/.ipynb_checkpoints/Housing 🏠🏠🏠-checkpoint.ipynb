{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359a9680",
   "metadata": {},
   "source": [
    "## My project on \"Hand on ML with Keras Tensorflow and Sci-kit learn By Aurelien\" \n",
    "### End-to-End Machine Learning ProjectðŸ§ŠðŸ§ŠðŸ§ŠðŸ§ŠðŸ§Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data(housing_path='./datasets/housing/'):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "housing_df = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fce16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c4335",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_df.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfbc7c",
   "metadata": {},
   "source": [
    "###### This md note is just for you to see how `np.random.permutation()` works:\n",
    "*** \n",
    "It basically takes in an integer and gives a randomly arranged set of numbers which starts from zero up until the integer that number specified \n",
    "***\n",
    "```\n",
    "randstuff = np.random.permutation(10)\n",
    "print(randstuff)\n",
    "Output: array([9, 4, 1, 7, 8, 5, 0, 6, 2, 3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7545f1",
   "metadata": {},
   "source": [
    "- Let's seperate our training set from our test set!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e6f0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, test_ratio):\n",
    "    shuffled_indexes = np.random.permutation(len(data))\n",
    "    test_size = int(len(data) * test_ratio)\n",
    "    test_set_indexes = shuffled_indexes[:test_size]\n",
    "    train_set_indexes = shuffled_indexes[test_size:]\n",
    "    return [data.iloc[train_set_indexes], data.iloc[test_set_indexes]]\n",
    "    print('''\n",
    "        Usage: train_set, test_set = split_train_test(data, test_ratio)\n",
    "        data: A pandas dataframe....\n",
    "        test_ratio: Should be in the range of [0 - 1]\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60769022",
   "metadata": {},
   "source": [
    "```\n",
    "Well, the function above works well but there's still a problem here.\n",
    "if we run the program again, we will get a different test set and overtime, your ML algorithm will see the whole dataset which is what me and you know you want to avoid ðŸ˜ŽðŸ˜ŽðŸ˜Ž.\n",
    "One solution is to save the testset on the firat run, and load it subsequently. Another option is to set the random number generator's seed (e.g np.random.seed(42)) before calling np.random.permutation(), so that it always generates the same shuffled indexes.\n",
    "bUt the issue is, both soluutions above will fail if we fetch an updated dataset!!\n",
    "\n",
    "A common solution is to use each instance's identifier to decide whether or not it should go\n",
    "in the test set (assuming instances have a unique and immutable identifier). For\n",
    "example, you could compute a hash of each instance's identifier and put that instance\n",
    "in the test set if the hash is lower or equal to 20% of the maximum hash value. This\n",
    "ensures that the test set will remain consistent across multiple runs, even if you\n",
    "refresh the dataset. The new test set will contain 20% of the new instances, but it will\n",
    "not contain any instance that was previously in the training set. Here is how it is implemented\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column_name):\n",
    "    ids = data[id_column_name]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return [data.loc[~in_test_set], data.loc[in_test_set]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe4e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing_with_id = housing_df.reset_index() # adds an `index` column\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8259e",
   "metadata": {},
   "source": [
    "```\n",
    "If you use the row index as a unique identifier, you need to make sure that new data\n",
    "gets appended to the end of the dataset, and no row ever gets deleted. If this is not\n",
    "possible, then you can try to use the most stable features to build a unique identifier.\n",
    "For example, a districtâ€™s latitude and longitude are guaranteed to be stable for a few\n",
    "million years, so you could combine them into an ID like so:\n",
    "```\n",
    "\n",
    "***\n",
    "`housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]`\n",
    "`train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")`\n",
    "***\n",
    "\n",
    "```\n",
    "Scikit-Learn provides a few functions to split datasets into multiple subsets in various\n",
    "ways. The simplest function is train_test_split, which does pretty much the same\n",
    "thing as the function split_train_test defined earlier, with a couple of additional\n",
    "features. First there is a random_state parameter that allows you to set the random\n",
    "generator seed as explained previously, and second you can pass it multiple datasets\n",
    "with an identical number of rows, and it will split them on the same indices (this is\n",
    "very useful, for example, if you have a separate DataFrame for labels):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a659ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(housing_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a31d70",
   "metadata": {},
   "source": [
    "***\n",
    "```\n",
    "Suppose you chatted with experts who told you that the median income is a very\n",
    "important attribute to predict median housing prices. You may want to ensure that\n",
    "the test set is representative of the various categories of incomes in the whole dataset.\n",
    "Since the median income is a continuous numerical attribute, you first need to create\n",
    "an income category attribute. Let's look at the median income histogram more closely\n",
    "(The histogram at the beginning of the notebook): most median income values are clustered around 1.5 to 6 (i.e.,\n",
    "$15,000 - $60,000), but some median incomes go far beyond 6. It is important to have\n",
    "a sufficient number of instances in your dataset for each stratum, or else the estimate\n",
    "of the stratum's importance may be biased. This means that you should not have too\n",
    "many strata, and each stratum should be large enough. The following code uses the\n",
    "pd.cut() function to create an income category attribute with 5 categories (labeled\n",
    "from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from\n",
    "1.5 to 3, and so on:\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccfd726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing_df[\"income_categories\"] = pd.cut(\n",
    "    housing_df[\"median_income\"],\n",
    "    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ")\n",
    "housing_df[\"income_categories\"].hist()\n",
    "plt.figure(figsize=(1, 2))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7caaad6",
   "metadata": {},
   "source": [
    "- Notice that a new column has been added to our dataframe ðŸ§ŠðŸ§ŠðŸ§ŠðŸ§Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6a5a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0fc755",
   "metadata": {},
   "source": [
    "***\n",
    "Now you are ready to do `stratified sampling` based on the income category. For this you can use Scikit-Learn's `StratifiedShuffleSplit` class:\n",
    "***\n",
    "- Stratisfied sampling is all about taking data from all strata(categories) into consideration while building an ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d321d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd3f6f",
   "metadata": {},
   "source": [
    "- Let's see what is inside `split.split(housing_df, housing_df[\"income_categories\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63dad86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for x, y in split.split(housing_df, housing_df[\"income_categories\"]):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(len(x))\n",
    "    print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in split.split(housing_df, housing_df[\"income_categories\"]):\n",
    "    strat_train_set = housing_df.loc[train_index]\n",
    "    strat_test_set = housing_df.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581097f",
   "metadata": {},
   "source": [
    "***\n",
    "```\n",
    "Letâ€™s see if this worked as expected. You can start by looking at the income category proportions in both the test and train sets.\n",
    "\n",
    "Notice that the propotion of each category in the two sets are very similar(if not the same).\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e46fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strat_test_set[\"income_categories\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c277d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "strat_train_set[\"income_categories\"].value_counts() / len(strat_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9d4fd",
   "metadata": {},
   "source": [
    "- Now that we have engineered our train and test dataframes using stratisfied sampling on our `median income` column, we can safely drop the `income_categories` column we created the other time!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0761129",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in [strat_test_set, strat_train_set]:\n",
    "    dataframe.drop([\"income_categories\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a87762",
   "metadata": {},
   "source": [
    "***\n",
    "```\n",
    "We spent quite a bit of time on test set generation for a good reason: this is an often\n",
    "neglected but critical part of a Machine Learning project. Moreover, many of these\n",
    "ideas will be useful later when we discuss cross-validation. Now itâ€™s time to move on\n",
    "to the next stage: exploring the data.\n",
    "\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29011936",
   "metadata": {},
   "source": [
    "## Discover and Visualize the Data to Gain Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224d209",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_housing_df = strat_train_set.copy()\n",
    "new_housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ccd7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_housing_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec2b8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_housing_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d0c9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_housing_df.plot(\n",
    "    kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=new_housing_df[\"population\"]/100,\n",
    "    label=\"population\", figsize=(10, 7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), \n",
    "    colorbar=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cacef21",
   "metadata": {},
   "source": [
    "## Looking for Correlations\n",
    "```\n",
    "Since the dataset is not too large you can easily compute the standard correlation\n",
    "coefficient (also called Pearson's r) between every pair of attributes using the corr()\n",
    "method:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c424e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlation = new_housing_df.corr(numeric_only=True)\n",
    "correlation[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93666cf4",
   "metadata": {},
   "source": [
    "### Experimenting with Attribute Combinations\n",
    "```\n",
    "Hopefully the previous sections gave you an idea of a few ways you can explore the\n",
    "data and gain insights. You identified a few data quirks that you may want to clean up\n",
    "before feeding the data to a Machine Learning algorithm, and you found interesting\n",
    "correlations between attributes, in particular with the target attribute. You also\n",
    "noticed that some attributes have a tail-heavy distribution, so you may want to transâ€\n",
    "form them (e.g., by computing their logarithm). Of course, your mileage will vary\n",
    "considerably with each project, but the general ideas are similar.\n",
    "One last thing you may want to do before actually preparing the data for Machine\n",
    "Learning algorithms is to try out various attribute combinations. For example, the\n",
    "total number of rooms in a district is not very useful if you donâ€™t know how many\n",
    "households there are. What you really want is the number of rooms per household.\n",
    "Similarly, the total number of bedrooms by itself is not very useful: you probably\n",
    "want to compare it to the number of rooms. And the population per household also\n",
    "seems like an interesting attribute combination to look at. Letâ€™s create these new\n",
    "attributes:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e892593",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_housing_df[\"rooms_per_household\"] = new_housing_df[\"total_rooms\"]/new_housing_df[\"households\"]\n",
    "new_housing_df[\"bedrooms_per_room\"] = new_housing_df[\"total_bedrooms\"]/new_housing_df[\"total_rooms\"]\n",
    "new_housing_df[\"population_per_household\"]=new_housing_df[\"population\"]/new_housing_df[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e33c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = new_housing_df.corr(numeric_only=True)\n",
    "correlation[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9385e",
   "metadata": {},
   "source": [
    "***\n",
    "```\n",
    "We can see that the columns we engineered has more correlation with the median_house_value than the actual column we engineered them from!! This means that we can actually add our enigneered columns into our dataframe if those columns have a reasonable correlation with our label, which is median_house_value in this case\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb03b86",
   "metadata": {},
   "source": [
    "## Prepare the Data for Machine Learning Algorithms\n",
    "It's time to prepare the data for your Machine Learning algorithms. Instead of just\n",
    "doing this manually, you should write functions to do that, for several good reasons:\n",
    "- This will allow you to reproduce these transformations easily on any dataset (e.g.,\n",
    "the next time you get a fresh dataset).\n",
    "- You will gradually build a library of transformation functions that you can reuse\n",
    "in future projects.\n",
    "- You can use these functions in your live system to transform the new data before\n",
    "feeding it to your algorithms.\n",
    "- This will make it possible for you to easily try various transformations and see which combination of transformations works best.\n",
    "```\n",
    "But first letâ€™s revert to a clean training set (by copying strat_train_set once again),\n",
    "and letâ€™s separate the predictors and the labels since we donâ€™t necessarily want to apply\n",
    "the same transformations to the predictors and the target values (note that drop()\n",
    "creates a copy of the data and does not affect strat_train_set):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ad401",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Many datasets use to have some missing values and if not controlled well, it can negatively affect out model.\n",
    "\n",
    "#### There are three common ways to solve this problem:\n",
    "1. Get rid of the whole row that has a missing column in it. we do this using df.dropna([missing_column])\n",
    "2. Get rid of a whole column. We do this by df.drop(column, axis=1)\n",
    "3. Set the values to some value (zero, the mean, the median, etc.). We do this using df[column].fillna()\n",
    "\n",
    "Sklearn provides a handy class to take care of missing values. it it SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d8e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "# Since the median can only be computed on numerical attributes, we need to create a copy of the data without the text attribute ocean_proximity:\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b296b8",
   "metadata": {},
   "source": [
    "- To accees the median value that was used to fill missing values, you can use the instance variable `statistics_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731f4b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3b797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputer.strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc57f4",
   "metadata": {},
   "source": [
    "- Let's turn the data in th `ocean_proximity` column to integers using `oneHotEncoder()`\n",
    "- **Note:** `oneHotEncoder()` works like `pd.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2594b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25690ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder_obj = OneHotEncoder()\n",
    "ocean_prox_encoded = encoder_obj.fit_transform(housing[[\"ocean_proximity\"]])\n",
    "ocean_prox_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba2c38",
   "metadata": {},
   "source": [
    "##### ***\n",
    "```\n",
    "Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very\n",
    "useful when you have categorical attributes with thousands of categories. After one-hot encoding we get a matrix with thousands of columns, and the matrix is full of\n",
    "zeros except for a single 1 per row. Using up tons of memory mostly to store zeros\n",
    "would be very wasteful, so instead a sparse matrix only stores the location of the nonâ€zero elements. You can use it mostly like a normal 2D array,21 but if you really want to\n",
    "convert it to a (dense) NumPy array, just call the toarray() method\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4171fad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ocean_prox_encoded.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e9423",
   "metadata": {},
   "source": [
    "### Custom Transformers\n",
    "Although Scikit-Learn provides many useful transformers, you will need to write\n",
    "your own for tasks such as custom cleanup operations or combining specific\n",
    "attributes. You will want your transformer to work seamlessly with Scikit-Learn funcâ€\n",
    "tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inherâ€\n",
    "itance), all you need is to create a class and implement three methods: fit()\n",
    "(returning self), transform(), and fit_transform(). You can get the last one for\n",
    "free by simply adding TransformerMixin as a base class. Also, if you add BaseEstima\n",
    "tor as a base class (and avoid *args and ** kwargs in your constructor) you will get\n",
    "two extra methods (get_params() and set_params()) that will be useful for autoâ€matic hyperparameter tuning. For example, here is a small transformer class that adds\n",
    "the combined attributes we discussed earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85879eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "class CombinedAttributeAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X,  rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d555772",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "Many machine leearning algorithms don't use to perform well when our training set has features of very different scales.\n",
    "To solve this problem, we have to rescale our features and there are two commons ways to get all our features to have the same scale.\n",
    "1. Min-max scaling (also known as `normalization`)\n",
    "2. standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90a225",
   "metadata": {},
   "source": [
    "### Transformation Pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60acfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('feature_adder', CombinedAttributeAdder()),\n",
    "    ('standard_scaler', StandardScaler()),\n",
    "])\n",
    "housing_num_tr = transform_pipeline.fit_transform(housing_num)\n",
    "print(housing_num.shape)\n",
    "housing_num_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443f69e",
   "metadata": {},
   "source": [
    "- Notice that we now have 11 columns after working with the pipilene. This is because the `CombinedAttributeAdder()` in our pipeline actually added three more colums to our datasets..\n",
    "\n",
    "***For real, pipelines are cool ðŸ§ŠðŸ§ŠðŸ§Š !!!***\n",
    "\n",
    "When you call the pipelineâ€™s fit() method, it calls fit_transform() sequentially on\n",
    "all transformers, passing the output of each call as the parameter to the next call, until\n",
    "it reaches the final estimator, for which it just calls the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed59fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7fbc6",
   "metadata": {},
   "source": [
    "So far, we have handled the categorical columns and the numerical columns sepaâ€\n",
    "rately. It would be more convenient to have a single transformer able to handle all colâ€\n",
    "umns, applying the appropriate transformations to each column. In version 0.20,\n",
    "Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news\n",
    "is that it works great with Pandas DataFrames. Letâ€™s use it to apply all the transformaâ€\n",
    "tions to the housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d90650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", transform_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs)\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbb016",
   "metadata": {},
   "source": [
    "### Select and Train a Model\n",
    "At last! You framed the problem, you got the data and explored it, you sampled a\n",
    "training set and a test set, and you wrote transformation pipelines to clean up and\n",
    "prepare your data for Machine Learning algorithms automatically. You are now ready\n",
    "to select and train a Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef3f54",
   "metadata": {},
   "source": [
    "- #### Letâ€™s first train a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14eeb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regresssor = LinearRegression()\n",
    "\n",
    "regresssor.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdcfda5",
   "metadata": {},
   "source": [
    "- Now that we have trained our model with `regressor.fit()`, Let's use it to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388b88a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "some_test_data = housing.iloc[:5, :]\n",
    "some_test_data_prepared = full_pipeline.transform(some_test_data)\n",
    "some_label_data = housing_labels.iloc[:5]\n",
    "print(some_label_data)\n",
    "some_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673f2f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Predictions: {regresssor.predict(some_test_data_prepared)}\")\n",
    "print(f\"Labels: {list(some_label_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set\n",
    "test_set_attrib = strat_test_set.drop([\"median_house_value\"], axis=1)\n",
    "test_set_label = strat_test_set[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_attrib_prepared = full_pipeline.transform(test_set_attrib)\n",
    "test_set_attrib_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "regresssor.score(test_set_attrib_prepared, test_set_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdfc2c",
   "metadata": {},
   "source": [
    " - Yhayy!! we have have trained our model and tested it. Although, our model accuracy is still bad. \n",
    " Let's measure the RMSE on the whole training data to visualize what is realy going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbba83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa316b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = regresssor.predict(some_test_data_prepared)\n",
    "lin_mse = mean_squared_error(some_label_data, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478caba",
   "metadata": {},
   "source": [
    "    - With this, we can see that our current model (let's say model 1.0) is not a good median_house_price predictor a most districts' median_housing_values range between `$120,000` and `$265,000`, so a typical prediction error of `$47159.2238` is not very satisfying\n",
    "        \n",
    "***This is an example of the model underfitting the training data***\n",
    "\n",
    "#### As we have seen in the previous lessons, the main ways to fix underfitting are:\n",
    "1. To select a more powerful model\n",
    "2. To feed the training algorithm with better features\n",
    "3. To reduce the constraints on the model\n",
    "\n",
    "Note: This model is not regularized, so this rules\n",
    "out the last option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96af04",
   "metadata": {},
   "source": [
    "- Letâ€™s train a DecisionTreeRegressor. This is a powerful model, capable of finding complex nonlinear relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d04d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "tree_regressor.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_predictions = tree_regressor.predict(housing_prepared)\n",
    "print(f\"Predictions: {list(tree_predictions)[:10]}\")\n",
    "print(f\"Labels: {list(housing_labels)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfad2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_mse = mean_squared_error(housing_labels, tree_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae1e6d",
   "metadata": {},
   "source": [
    "```\n",
    "Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,\n",
    "it is much more likely that the model has badly overfit the data. How can you be sure?\n",
    "As we saw earlier, you donâ€™t want to touch the test set until you are ready to launch a\n",
    "model you are confident about, so you need to use part of the training set for trainâ€\n",
    "ing, and part for model validation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a random plot hahahaha!!!!\n",
    "plt.plot(housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7fefb",
   "metadata": {},
   "source": [
    "```\n",
    "One way to evaluate the Decision Tree model would be to use the train_test_split\n",
    "function to split the training set into a smaller training set and a validation set, then\n",
    "train your models against the smaller training set and evaluate them against the valiâ€\n",
    "dation set. Itâ€™s a bit of work, but nothing too difficult and it would work fairly well.\n",
    "A great alternative is to use Scikit-Learnâ€™s K-fold cross-validation feature. The followâ€\n",
    "ing code randomly splits the training set into 10 distinct subsets called folds, then it\n",
    "trains and evaluates the Decision Tree model 10 times, picking a different fold for\n",
    "evaluation every time and training on the other 9 folds. The result is an array conâ€\n",
    "taining the 10 evaluation scores:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c765d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "tree_scores = cross_val_score(tree_regressor, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "list(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beac33c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81150a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly as ply\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "N = 100\n",
    "random_x = np.linspace(0, 1, N)\n",
    "random_y0 = np.random.randn(N)+5\n",
    "random_y1 = np.random.randn(N)\n",
    "random_y2 = np.random.randn(N)-5\n",
    "# Create traces\n",
    "trace0 = go.Scatter(\n",
    " x = random_x,\n",
    "y = random_y0,\n",
    "mode = 'lines',\n",
    "name = 'lines'\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    " x = random_x,\n",
    " y = random_y1,\n",
    " mode = 'lines+markers',\n",
    " name = 'lines+markers'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    " x = random_x,\n",
    " y = random_y2,\n",
    " mode = 'markers',\n",
    " name = 'markers'\n",
    ")\n",
    "data = [trace0, trace1, trace2]\n",
    "ply.offline.plot(data, filename='line-mode.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the decision tree algorithm when you figure out how and why it is said to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09927b15",
   "metadata": {},
   "source": [
    "```\n",
    "Random forest works by training many decision trees on random subsets of the features, then averaging out their predictons.\n",
    "```\n",
    "***Building a model on top of many other models is called ensemble Learning and it is often a great way to push ML algorithms even further.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98911516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_regressor = RandomForestRegressor()\n",
    "forest_regressor.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_predictions = forest_regressor.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, forest_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_rmse_scores = cross_val_score(forest_regressor, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "display_scores(forest_rmse_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
